{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import cv2\n",
    "import random\n",
    "import time\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# python hyperparameters\n",
    "LEARNING_RATE = 0.0001\n",
    "EPOCH = 100\n",
    "CLASS_NUM = 5\n",
    "\n",
    "# const\n",
    "CHANEL = 1\n",
    "res_stru = {'50':[3,4,6,3],\n",
    "            '101':[3,4,23,3],\n",
    "            '152':[3,8,36,3]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# colect data\n",
    "# def process_data_set():\n",
    "#     TRAIN_N_PATH = './data/train/0'\n",
    "#     TRAIN_D_PATH = './data/train/1'\n",
    "#     TRAIN_W_PATH = './data/train/2'\n",
    "#     TRAIN_Z_PATH = './data/train/3'\n",
    "#     TRAIN_P_PATH = './data/train/4'\n",
    "    \n",
    "#     TEST_N_PATH = './data/test/0'\n",
    "#     TEST_D_PATH = './data/test/1'\n",
    "    \n",
    "#     train_n = glob.glob(TRAIN_N_PATH+'/*.*')\n",
    "#     train_d = glob.glob(TRAIN_D_PATH+'/*.*')\n",
    "#     train_w = glob.glob(TRAIN_W_PATH+'/*.*')\n",
    "#     train_z = glob.glob(TRAIN_Z_PATH+'/*.*')\n",
    "#     train_p = glob.glob(TRAIN_P_PATH+'/*.*')\n",
    "    \n",
    "#     test_n = glob.glob(TEST_N_PATH+'/*.*')\n",
    "#     test_d = glob.glob(TEST_D_PATH+'/*.*')\n",
    "    \n",
    "#     train_feature = train_n+train_d+train_w+train_z+train_p\n",
    "#     train_label = [0 for i in range(len(train_n))] + \\\n",
    "#             [1 for i in range(len(train_d))] + \\\n",
    "#             [2 for i in range(len(train_w))] + \\\n",
    "#             [3 for i in range(len(train_z))] + \\\n",
    "#             [4 for i in range(len(train_p))]\n",
    "            \n",
    "#     test_feature = test_n+test_d\n",
    "#     test_label = [0 for i in range(len(test_n))] + \\\n",
    "#                  [1 for i in range(len(test_d))]\n",
    "    \n",
    "#     # 对数据进行清洗\n",
    "#     shuffle_index = [i for i in range(len(train_feature))]\n",
    "#     np.random.shuffle(shuffle_index)\n",
    "#     train_x = np.array(train_feature)[shuffle_index]\n",
    "#     train_y = np.array(train_label)[shuffle_index]\n",
    "\n",
    "#     shuffle_index = [i for i in range(len(test_feature))]\n",
    "#     np.random.shuffle(shuffle_index)\n",
    "#     test_x = np.array(test_feature)[shuffle_index]\n",
    "#     test_y = np.array(test_label)[shuffle_index]\n",
    "#     valid_x = np.concatenate((train_x,train_x,train_x),axis=0)[:1000]\n",
    "#     valid_y = np.concatenate((train_y,train_y,train_y),axis=0)[:1000]\n",
    "    \n",
    "#     train_y = np.eye(CLASS_NUM)[train_y]\n",
    "#     valid_y = np.eye(CLASS_NUM)[valid_y]\n",
    "#     test_y = np.eye(CLASS_NUM)[test_y]\n",
    "#     return train_x,train_y,valid_x,valid_y,test_x,test_y\n",
    "\n",
    "\n",
    "def process_data_set():\n",
    "    TRAIN_N_PATH = './data/train/0'\n",
    "    TRAIN_D_PATH = './data/train/1'\n",
    "    TRAIN_W_PATH = './data/train/2'\n",
    "    TRAIN_Z_PATH = './data/train/3'\n",
    "    TRAIN_P_PATH = './data/train/4'\n",
    "\n",
    "    TEST_N_PATH = './data/test/0'\n",
    "    TEST_D_PATH = './data/test/1'\n",
    "    TEST_W_PATH = './data/test/2'\n",
    "    TEST_Z_PATH = './data/test/3'\n",
    "    TEST_P_PATH = './data/test/4'\n",
    "\n",
    "    train_n = glob.glob(TRAIN_N_PATH + '/*.*')\n",
    "    train_d = glob.glob(TRAIN_D_PATH + '/*.*')\n",
    "    train_w = glob.glob(TRAIN_W_PATH + '/*.*')\n",
    "    train_z = glob.glob(TRAIN_Z_PATH + '/*.*')\n",
    "    train_p = glob.glob(TRAIN_P_PATH + '/*.*')\n",
    "\n",
    "    test_n = glob.glob(TEST_N_PATH + '/*.*')\n",
    "    test_d = glob.glob(TEST_D_PATH + '/*.*')\n",
    "    test_w = glob.glob(TEST_W_PATH + '/*.*')\n",
    "    test_z = glob.glob(TEST_Z_PATH + '/*.*')\n",
    "    test_p = glob.glob(TEST_P_PATH + '/*.*')\n",
    "    train_feature = train_n + train_d + train_w + train_z + train_p\n",
    "    train_label = [0 for i in range(len(train_n))] + \\\n",
    "                  [1 for i in range(len(train_d))] + \\\n",
    "                  [2 for i in range(len(train_w))] + \\\n",
    "                  [3 for i in range(len(train_z))] + \\\n",
    "                  [4 for i in range(len(train_p))]\n",
    "\n",
    "    test_feature = test_n + test_d + test_w + test_z + test_p\n",
    "    test_label = [0 for i in range(len(test_n))] + \\\n",
    "                 [1 for i in range(len(test_d))] + \\\n",
    "                 [2 for i in range(len(test_w))] + \\\n",
    "                 [3 for i in range(len(test_z))] + \\\n",
    "                 [4 for i in range(len(test_p))]\n",
    "\n",
    "\n",
    "    # 对数据进行清洗\n",
    "    shuffle_index = [i for i in range(len(train_feature))]\n",
    "    np.random.shuffle(shuffle_index)\n",
    "    train_x = np.array(train_feature)[shuffle_index]\n",
    "    train_y = np.array(train_label)[shuffle_index]\n",
    "\n",
    "    shuffle_index = [i for i in range(len(test_feature))]\n",
    "    np.random.shuffle(shuffle_index)\n",
    "    test_x = np.array(test_feature)[shuffle_index]\n",
    "    test_y = np.array(test_label)[shuffle_index]\n",
    "    valid_x = np.concatenate((train_x, train_x, train_x), axis=0)[:1000]\n",
    "    valid_y = np.concatenate((train_y, train_y, train_y), axis=0)[:1000]\n",
    "\n",
    "    train_y = np.eye(CLASS_NUM)[train_y]\n",
    "    valid_y = np.eye(CLASS_NUM)[valid_y]\n",
    "    test_y = np.eye(CLASS_NUM)[test_y]\n",
    "    return train_x, train_y, valid_x, valid_y, test_x, test_y\n",
    "\n",
    "assert process_data_set()[-3].shape == (1000,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train data augmentation\n",
    "def image_augment_from_path(image_path):\n",
    "    img = cv2.imread(image_path,0)\n",
    "    # 这里要根据channel 改变图像\n",
    "\n",
    "    # resize图片到336*x或者x*336或者336*336\n",
    "    target_size = 224\n",
    "    if img.shape[0] / img.shape[1] < 0.9:\n",
    "        resize_factor = target_size / img.shape[0]\n",
    "        width = np.int32(img.shape[1] * resize_factor)\n",
    "        img = cv2.resize(img,(width,target_size),interpolation=cv2.INTER_NEAREST)\n",
    "    elif img.shape[0] / img.shape[1] > 1.11:\n",
    "        resize_factor = target_size / img.shape[1]\n",
    "        height = np.int32(img.shape[0] * resize_factor,interpolation=cv2.INTER_NEAREST)\n",
    "        img = cv2.resize(img,(target_size,height))\n",
    "    else:\n",
    "        # todo 这里变形了 应该用padding\n",
    "        img = cv2.resize(img,(target_size,target_size),interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "    # 随机裁剪区域\n",
    "    v_start = np.int(random.uniform(0,img.shape[0]-224))\n",
    "    h_start = np.int(random.uniform(0,img.shape[1]-224))\n",
    "    img = img[v_start:v_start+224,h_start:h_start+224]\n",
    "\n",
    "    # 随机水平或者垂直翻转\n",
    "    is_flip_v = bool(random.getrandbits(1))\n",
    "    is_flip_h = bool(random.getrandbits(1))\n",
    "    if is_flip_v and is_flip_h:\n",
    "        img = cv2.flip(img,-1)\n",
    "    elif (is_flip_v and (not is_flip_h)):\n",
    "        img = cv2.flip(img,0)\n",
    "    elif ((not is_flip_v) and is_flip_h):\n",
    "        img = cv2.flip(img,1)\n",
    "    a = np.random.randint(80, 120) / 100;\n",
    "    b = np.random.randint(-20, 20);\n",
    "    img = np.uint8(np.clip((a * img + b), 0, 255))\n",
    "    img = np.reshape(img,(224,224,1))\n",
    "    return img\n",
    "\n",
    "def gen_batch(x,y,batch_size):\n",
    "    if len(x) % batch_size == 0:\n",
    "        batch_num = len(x) // batch_size\n",
    "    else:\n",
    "        batch_num = len(x) // batch_size + 1\n",
    "    index = 0\n",
    "    for i in range(batch_num):\n",
    "        batch_path = x[index:index+batch_size]\n",
    "        batch_y = y[index:index+batch_size]\n",
    "        batch_x = [image_augment_from_path(batch_path[j]) for j in range(len(batch_path))]\n",
    "        yield batch_x,batch_y\n",
    "        index = index + batch_size\n",
    "        \n",
    "# import cv2\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# import random\n",
    "# %matplotlib inline\n",
    "# plt.imshow(np.reshape(image_augment_from_path('D:/teddy/teddy/project/A/data/test/00.jpg'),[224,224]),cmap='gray')\n",
    "# print(image_augment_from_path('D:/teddy/teddy/project/A/data/test/00.jpg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def show_train(y_list):\n",
    "    y = y_list\n",
    "    x = np.linspace(1,len(y_list),len(y_list))\n",
    "    plt.plot(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test image （graph）\n",
    "def test_image_stream(image_stream,width,height):\n",
    "\n",
    "#     img = tf.read_file(path_holder)\n",
    "#     raw_img = tf.image.decode_jpeg(img,channels=CHANEL)\n",
    "    raw_image = tf.reshape(image_stream,[height,width,-1])\n",
    "\n",
    "    # 需要设计网络\n",
    "    # 3行*6列 尽量保持图片不变形\n",
    "    img = tf.image.resize_images(raw_image,[672,1344],method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "    num_width = tf.constant(6)\n",
    "    num_height = tf.constant(3)\n",
    "    cond = lambda i,num_width,num_height,img,imgs:tf.less(i,num_height)\n",
    "    def body(i,num_width,num_height,img,imgs):\n",
    "        j = tf.constant(0)\n",
    "        inner_cond = lambda i,j,num_width,num_height,img,imgs:tf.less(j,num_width)\n",
    "        def inner_body(i,j,num_width,num_height,img,imgs):\n",
    "            cut_img = img[i * 224: i * 224 + 224 ,j * 224 : j * 224 + 224,:]\n",
    "            cut_img = tf.reshape(cut_img,[1,224,224,CHANEL])\n",
    "            imgs = tf.concat([imgs,cut_img],0)\n",
    "            return i,j+1,num_width,num_height,img,imgs\n",
    "        i,j,num_width,num_height,img,imgs = tf.while_loop(inner_cond,\n",
    "                                                          inner_body,\n",
    "                                                          (i,j,num_width,num_height,img,imgs),\n",
    "                                                          (i.get_shape(),j.get_shape(),num_width.get_shape(),num_height.get_shape(),img.get_shape(),tf.TensorShape([None,224,224,CHANEL])))\n",
    "        return i+1,num_width,num_height,img,imgs\n",
    "    i = tf.constant(0)\n",
    "    imgs = tf.zeros([0,224,224,CHANEL],tf.uint8)\n",
    "    i,num_width,num_height,img,imgs = tf.while_loop(cond,\n",
    "                                                    body,\n",
    "                                                    (i,num_width,num_height,img,imgs),\n",
    "                                                    (i.get_shape(),num_width.get_shape(),num_height.get_shape(),img.get_shape(),tf.TensorShape([None,224,224,CHANEL])))\n",
    "    \n",
    "    \n",
    "    return imgs\n",
    "\n",
    "# CHANEL = 1\n",
    "# import tensorflow as tf\n",
    "# import matplotlib.pyplot as plt\n",
    "# import cv2\n",
    "# import numpy as np\n",
    "# %matplotlib inline\n",
    "# image_ = tf.placeholder(tf.uint8)\n",
    "# width_ = tf.placeholder(tf.int32)\n",
    "# height_ = tf.placeholder(tf.int32)\n",
    "# sess = tf.InteractiveSession()\n",
    "# img = np.reshape(cv2.imread('D:/teddy/teddy/project/A/data/test/test.jpg',0),[-1])\n",
    "# imgs = test_image_stream(image_,width_,height_)\n",
    "# tf.global_variables_initializer().run()\n",
    "# imgs_tensor = sess.run(imgs,feed_dict={image_:img,width_:800,height_:503})\n",
    "# plt.imshow(np.reshape(imgs_tensor[4],(224,224)), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def input_holder():\n",
    "#     x_test_path_ = tf.placeholder(tf.string,name='image_address')\n",
    "    y_ = tf.placeholder(tf.float32,(None,CLASS_NUM),name='y_input')\n",
    "    train_flag_ = tf.placeholder(tf.bool,name='train_flag')\n",
    "    image_ = tf.placeholder(tf.uint8,name='raw_image')\n",
    "    width_ = tf.placeholder(tf.int32,name='width')\n",
    "    height_ = tf.placeholder(tf.int32,name='height')\n",
    "    return y_,train_flag_,image_,width_,height_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 残差网络\n",
    "def convolutional_block(x,filters,is_train):\n",
    "    short_cut = tf.layers.conv2d(x,filters*4,1,2,padding='same')\n",
    "    x = tf.layers.conv2d(x,filters,1,2,activation=tf.nn.relu,padding='same')\n",
    "    x = tf.layers.batch_normalization(x,training=is_train)\n",
    "    x = tf.layers.conv2d(x,filters,3,1,activation=tf.nn.relu,padding='same')\n",
    "    x = tf.layers.batch_normalization(x,training=is_train)\n",
    "    x = tf.layers.conv2d(x,filters*4,1,1,padding='same')\n",
    "    x = tf.layers.batch_normalization(x,training=is_train)\n",
    "    x = x+short_cut\n",
    "    x = tf.nn.relu(x)\n",
    "    return x\n",
    "    \n",
    "def identity_block(x,filters,is_train):\n",
    "    short_cut = x\n",
    "    x = tf.layers.conv2d(x,filters,1,1,activation=tf.nn.relu,padding='same')\n",
    "    x = tf.layers.batch_normalization(x,training=is_train)\n",
    "    x = tf.layers.conv2d(x,filters,3,1,activation=tf.nn.relu,padding='same')\n",
    "    x = tf.layers.batch_normalization(x,training=is_train)\n",
    "    x = tf.layers.conv2d(x,filters*4,1,1,padding='same')\n",
    "    x = tf.layers.batch_normalization(x,training=is_train)\n",
    "    x = x+short_cut\n",
    "    x = tf.nn.relu(x)\n",
    "    return x\n",
    "\n",
    "def identity_block_depth(x,filters,is_train):\n",
    "    short_cut = tf.layers.conv2d(x,filters*4,1,1,padding='same')\n",
    "    x = tf.layers.conv2d(x,filters,1,1,activation=tf.nn.relu,padding='same')\n",
    "    x = tf.layers.batch_normalization(x,training=is_train)\n",
    "    x = tf.layers.conv2d(x,filters,3,1,activation=tf.nn.relu,padding='same')\n",
    "    x = tf.layers.batch_normalization(x,training=is_train)\n",
    "    x = tf.layers.conv2d(x,filters*4,1,1,padding='same')\n",
    "    x = tf.layers.batch_normalization(x,training=is_train)\n",
    "    x = x+short_cut\n",
    "    x = tf.nn.relu(x)\n",
    "    return x\n",
    "    \n",
    "def big_identity_block(x,block_n,is_train):\n",
    "    x = identity_block_depth(x,64,is_train)\n",
    "    for i in range(block_n):\n",
    "        x = identity_block(x,64,is_train)\n",
    "    return x\n",
    "    \n",
    "def big_convolutional_block(x,block_n,filters,is_train):\n",
    "    x = convolutional_block(x,filters,is_train)\n",
    "    for i in range(block_n-1):\n",
    "        x = identity_block(x,filters,is_train)\n",
    "    return x\n",
    "\n",
    "def resnet(x,is_train,depth='50'):\n",
    "    x = x / 255 - 0.5\n",
    "    x = tf.layers.conv2d(x,64,7,2,padding='same',name='base_cnn')\n",
    "    x = tf.layers.batch_normalization(x,name='base_bn',training=is_train)\n",
    "    x = tf.nn.relu(x)\n",
    "    x = tf.layers.max_pooling2d(x,2,2,name='base_pooling')\n",
    "    with tf.variable_scope('cfg0'):\n",
    "        x = big_identity_block(x,res_stru[depth][0],is_train)\n",
    "    with tf.variable_scope('cfg1'):\n",
    "        x = big_convolutional_block(x,res_stru[depth][1],128,is_train)\n",
    "    with tf.variable_scope('cfg2'):\n",
    "        x = big_convolutional_block(x,res_stru[depth][2],256,is_train)\n",
    "    with tf.variable_scope('cfg3'):\n",
    "        x = big_convolutional_block(x,res_stru[depth][3],512,is_train)\n",
    "    x = tf.layers.average_pooling2d(x,7,1)\n",
    "    x = tf.reshape(x,(-1,2048))\n",
    "    x = tf.layers.dense(x,1000,activation=tf.nn.relu)\n",
    "    logits = tf.layers.dense(x,CLASS_NUM,name='logits')\n",
    "    \n",
    "    return logits\n",
    "\n",
    "# VGG11层模型\n",
    "def vgg11(x,is_train):\n",
    "    # VGG 11层模型\n",
    "    x = x / 255 - 0.5\n",
    "    x = tf.layers.conv2d(x,64,3,1,padding='same',name='cnnlayer1')\n",
    "    x = tf.layers.batch_normalization(x,name='bn1',training=is_train)\n",
    "    x = tf.nn.relu(x)\n",
    "    x = tf.layers.max_pooling2d(x,2,2,padding='same',name='pooling1')\n",
    "    # 112*112*64\n",
    "    \n",
    "    x = tf.layers.conv2d(x,128,3,1,padding='same',name='cnnlayer2')\n",
    "    x = tf.layers.batch_normalization(x,name='bn2',training=is_train)\n",
    "    x = tf.nn.relu(x)\n",
    "    x = tf.layers.max_pooling2d(x,2,2,padding='same',name='pooling2')\n",
    "    # 56*64*128\n",
    "    \n",
    "    x = tf.layers.conv2d(x,256,3,1,padding='same',name='cnnlayer3')\n",
    "    x = tf.layers.batch_normalization(x,name='bn3',training=is_train)\n",
    "    x = tf.nn.relu(x)\n",
    "    x = tf.layers.conv2d(x,256,3,1,padding='same',name='cnnlayer4')\n",
    "    x = tf.layers.batch_normalization(x,name='bn4',training=is_train)\n",
    "    x = tf.nn.relu(x)\n",
    "    x = tf.layers.max_pooling2d(x,2,2,padding='same',name='pooling3')\n",
    "    # 28*28*256\n",
    "    \n",
    "    x = tf.layers.conv2d(x,512,3,1,padding='same',name='cnnlayer5')\n",
    "    x = tf.layers.batch_normalization(x,name='bn5',training=is_train)\n",
    "    x = tf.nn.relu(x)\n",
    "    x = tf.layers.conv2d(x,512,3,1,padding='same',name='cnnlayer6')\n",
    "    x = tf.layers.batch_normalization(x,name='bn6',training=is_train)\n",
    "    x = tf.nn.relu(x)\n",
    "    x = tf.layers.max_pooling2d(x,2,2,padding='same',name='pooling4') \n",
    "    # 14*14*512\n",
    "    \n",
    "    x = tf.layers.conv2d(x,512,3,1,padding='same',name='cnnlayer7')\n",
    "    x = tf.layers.batch_normalization(x,name='bn7',training=is_train)\n",
    "    x = tf.nn.relu(x)\n",
    "    x = tf.layers.conv2d(x,512,3,1,padding='same',name='cnnlayer8')\n",
    "    x = tf.layers.batch_normalization(x,name='bn8',training=is_train)\n",
    "    x = tf.nn.relu(x)\n",
    "    x = tf.layers.max_pooling2d(x,2,2,padding='same',name='pooling5') \n",
    "    # 7*7*512\n",
    "    \n",
    "    x = tf.reshape(x,[-1,7*7*512])\n",
    "    x = tf.layers.dense(x,4096,activation=tf.nn.relu,name='fc1')\n",
    "    x = tf.layers.dense(x,4096,activation=tf.nn.relu,name='fc2')\n",
    "    x = tf.layers.dense(x,1000,activation=tf.nn.relu,name='fc3')\n",
    "    logits = tf.layers.dense(x,CLASS_NUM,name='logits')\n",
    "    \n",
    "    return logits\n",
    "\n",
    "def cost_acc_step(logits,y):\n",
    "    output = tf.nn.softmax(logits)\n",
    "    output = tf.identity(output,name='output')\n",
    "    loss = tf.nn.softmax_cross_entropy_with_logits(logits=logits,labels=y,name='loss')\n",
    "    cost = tf.reduce_mean(loss,name='cost')\n",
    "    step = tf.train.AdamOptimizer(learning_rate=LEARNING_RATE).minimize(cost)\n",
    "    acc = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(output,axis=1),tf.argmax(y,axis=1)),tf.float32),name='acc')\n",
    "    return cost,acc,step,output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(model,epoch=EPOCH,batch_size=16,dev=False,checkpoint_path='tmp'):\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    # graph\n",
    "    y_,train_flag_,image_,width_,height_ = input_holder()\n",
    "    image = test_image_stream(image_,width_,height_)\n",
    "    x_ = tf.placeholder_with_default(image,(None,224,224,CHANEL),name='x_train')\n",
    "    \n",
    "    logits = resnet(x_,is_train=train_flag_)\n",
    "    cost,acc,step,output = cost_acc_step(logits,y_)\n",
    "    min_pred = tf.reduce_min(output)\n",
    "    pred = tf.identity(min_pred,name='pred')\n",
    "    \n",
    "    #which_box = tf.argmin(output,output_type=tf.int32)\n",
    "    which_box = tf.argmin(output)\n",
    "    which_box=tf.cast(which_box,dtype=tf.int32)\n",
    "    row = which_box // 6\n",
    "    column = which_box[0] % 6\n",
    "    points = [width_*column/6,height_*row/3,width_*(column+1)/6,height_*(row+1)/3]\n",
    "    points = tf.identity(points,name='points')\n",
    "    \n",
    "    \n",
    "    train_x,train_y,valid_x,valid_y,test_x,test_y = process_data_set()\n",
    "    \n",
    "    \n",
    "    sess = tf.InteractiveSession()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "    saver = tf.train.Saver(max_to_keep=20)\n",
    "    counter = 0\n",
    "    start_time = time.clock()\n",
    "    current_time = time.clock()\n",
    "    cost_list = []\n",
    "    acc_list = []\n",
    "    extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "    best_valid_acc = 0\n",
    "    saver.restore(sess,'./checkpoints/'+checkpoint_path+'zxq2max')\n",
    "    with tf.device(\"/gpu:0\"):\n",
    "        for i in range(epoch):\n",
    "            for x_batch,y_batch in gen_batch(train_x,train_y,batch_size):\n",
    "                print(\"counter=%d\"%counter)\n",
    "                if counter % 100 == 0:\n",
    "                    val_cost_list = []\n",
    "                    val_acc_list = []\n",
    "                    for  test_x_batch,test_y_batch in gen_batch(test_x,test_y,batch_size):\n",
    "                        step_test_cost,step_test_acc = sess.run([cost,acc],feed_dict={x_:test_x_batch,y_:test_y_batch,train_flag_:False})\n",
    "                        val_cost_list.append(step_test_cost)\n",
    "                        val_acc_list.append(step_test_acc)\n",
    "                    test_cost = np.mean(val_cost_list)\n",
    "                    test_acc = np.mean(val_acc_list)\n",
    "                    if best_valid_acc < test_acc:\n",
    "                        best_valid_acc = test_acc\n",
    "                        saver.save(sess,'./checkpoints/'+checkpoint_path+'zxq2max')\n",
    "\n",
    "                    time_per_step = time.clock() - current_time\n",
    "                    seconds = time_per_step * (epoch-i)*len(train_x) / (100*batch_size)\n",
    "                    m, s = divmod(seconds, 60)\n",
    "                    h, m = divmod(m, 60)\n",
    "                    eta = \"%02d:%02d:%02d\" % (h, m, s)\n",
    "                    current_time = time.clock()\n",
    "                    print('Step:{:>4d}, ETA:{}, Test_cost:{:.5F},best_valid_acc:{:.5F} ,Test_acc:{:.4F}'.format(counter,eta,test_cost,best_valid_acc,test_acc))\n",
    "\n",
    "                if counter % 500 == 0 and counter > 0:\n",
    "                    #saver.save(sess,'./checkpoints/'+checkpoint_path,global_step=i)\n",
    "                    print('checkpoint save as '+checkpoint_path+str(i))\n",
    "                _,__,step_cost,step_acc = sess.run([step,extra_update_ops,cost,acc],feed_dict={x_:x_batch,y_:y_batch,train_flag_:True})\n",
    "                cost_list.append(step_cost)\n",
    "                acc_list.append(step_acc)\n",
    "                counter = counter + 1\n",
    "                if dev and counter % 10 == 0:\n",
    "                    cur_acc = np.mean(acc_list[-10:])\n",
    "                    cur_cost = np.mean(cost_list[-10:])\n",
    "                    print(\"Step: \",counter,\" Cost: \",cur_cost,\" Accuracy: \",cur_acc)\n",
    "\n",
    "\n",
    "        sess.close()\n",
    "        show_train(acc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./checkpoints/reszxq2max\n",
      "counter=0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-faa55cb8d2c6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# load model + frezee\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0msess\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresnet\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcheckpoint_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'res'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdev\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-9-36f0a1befa69>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, epoch, batch_size, dev, checkpoint_path)\u001b[0m\n\u001b[0;32m     44\u001b[0m                     \u001b[0mval_acc_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m                     \u001b[1;32mfor\u001b[0m  \u001b[0mtest_x_batch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_y_batch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgen_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_y\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m                         \u001b[0mstep_test_cost\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mstep_test_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcost\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0macc\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mx_\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mtest_x_batch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mtest_y_batch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_flag_\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m                         \u001b[0mval_cost_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep_test_cost\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m                         \u001b[0mval_acc_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep_test_acc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\MyWorkSpace\\MyWork\\software\\Anaconda\\install\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    787\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 789\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    790\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\MyWorkSpace\\MyWork\\software\\Anaconda\\install\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    995\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    996\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m--> 997\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    998\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    999\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\MyWorkSpace\\MyWork\\software\\Anaconda\\install\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1130\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1131\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m-> 1132\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m   1133\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32mC:\\MyWorkSpace\\MyWork\\software\\Anaconda\\install\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1137\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1138\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1139\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1140\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1141\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\MyWorkSpace\\MyWork\\software\\Anaconda\\install\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1119\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[0;32m   1120\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1121\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m   1122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1123\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# load model + frezee\n",
    "sess = train(resnet,checkpoint_path='res',dev=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./checkpoints/res-19\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "moment_path = './checkpoints/res-19'\n",
    "file_path = './data/test/12.jpg'\n",
    "sess = tf.InteractiveSession()\n",
    "meta_path = moment_path+'.meta'\n",
    "saver = tf.train.import_meta_graph(meta_path,clear_devices=True)\n",
    "graph = tf.get_default_graph()\n",
    "saver.restore(sess,moment_path)\n",
    "#     train_flag_ = tf.placeholder(tf.bool,name='train_flag')\n",
    "#     image_ = tf.placeholder(tf.uint8,name='raw_image')\n",
    "#     width_ = tf.placeholder(tf.int32,name='width')\n",
    "#     height_ = tf.placeholder(tf.int32,name='height')\n",
    "image_address = graph.get_tensor_by_name('raw_image:0')\n",
    "train_flag = graph.get_tensor_by_name('train_flag:0')\n",
    "output = graph.get_tensor_by_name('output:0')\n",
    "# pred = graph.get_tensor_by_name('pred:0')\n",
    "width = graph.get_tensor_by_name('width:0')\n",
    "height = graph.get_tensor_by_name('height:0')\n",
    "# points = graph.get_tensor_by_name('points:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  7.02162147e-01   1.73124477e-01   3.41360969e-03   1.21209837e-01\n",
      "    8.99018050e-05]\n",
      " [  2.95585454e-01   8.20149407e-02   5.85517706e-03   6.16511464e-01\n",
      "    3.29670147e-05]\n",
      " [  1.74260467e-01   4.78960276e-02   2.76184548e-03   7.75069416e-01\n",
      "    1.22752072e-05]\n",
      " [  4.44533736e-01   2.60479860e-02   1.45224761e-03   5.27958155e-01\n",
      "    7.85023349e-06]\n",
      " [  8.83674482e-04   1.40902260e-03   5.32594684e-04   9.97171938e-01\n",
      "    2.88272486e-06]\n",
      " [  4.73569889e-05   3.43800813e-01   5.27708232e-01   3.16448174e-02\n",
      "    9.67987478e-02]\n",
      " [  2.53205289e-05   9.60888982e-01   1.74222561e-03   3.72375324e-02\n",
      "    1.05977364e-04]\n",
      " [  5.27900100e-01   4.72913794e-02   9.05375276e-03   4.15623128e-01\n",
      "    1.31667446e-04]\n",
      " [  1.91646034e-03   2.77255639e-03   9.46586609e-01   3.17488611e-02\n",
      "    1.69755053e-02]\n",
      " [  9.47986007e-01   4.50830162e-03   5.21765323e-04   4.69685234e-02\n",
      "    1.55224916e-05]\n",
      " [  8.72921228e-01   2.09565070e-02   2.98532471e-03   1.03026539e-01\n",
      "    1.10357883e-04]\n",
      " [  1.82855321e-04   9.93659198e-01   9.14366043e-04   4.47530579e-03\n",
      "    7.68298050e-04]\n",
      " [  4.38930601e-01   5.43726027e-01   8.52757541e-04   1.63001977e-02\n",
      "    1.90459832e-04]\n",
      " [  8.94907594e-01   4.75043207e-02   1.19533692e-03   5.63756451e-02\n",
      "    1.71070442e-05]\n",
      " [  9.96878624e-01   2.42813816e-03   4.78049406e-05   6.40803191e-04\n",
      "    4.70865780e-06]\n",
      " [  9.92587984e-01   6.37124060e-03   6.73627583e-05   9.66295018e-04\n",
      "    7.20371418e-06]\n",
      " [  9.88308430e-01   8.35950300e-03   1.12169793e-04   3.21105774e-03\n",
      "    8.81450251e-06]\n",
      " [  3.87637794e-01   2.82849342e-01   4.92955670e-02   2.76452035e-01\n",
      "    3.76529410e-03]]\n"
     ]
    }
   ],
   "source": [
    "file_path0 = './data/test2/00.jpg'\n",
    "import cv2 \n",
    "import numpy as np\n",
    "raw_image = cv2.imread(file_path0,0).flatten()\n",
    "xixi = sess.run([output],feed_dict = {image_address:raw_image,width:2592,height:1200,train_flag:False})\n",
    "print(np.min(xixi,axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Froze 324 variables.\n",
      "Converted 324 variables to const ops.\n",
      "1790 ops in the final graph.\n"
     ]
    }
   ],
   "source": [
    "output_node_names = 'raw_image,output,pred,train_flag,width,height'\n",
    "output_graph_def = tf.graph_util.convert_variables_to_constants(\n",
    "    sess, \n",
    "    tf.get_default_graph().as_graph_def(), \n",
    "    output_node_names.split(\",\") \n",
    ") \n",
    "\n",
    "output_graph = 'resnet38v1.pb'\n",
    "with tf.gfile.GFile(output_graph, \"wb\") as f:\n",
    "    f.write(output_graph_def.SerializeToString())\n",
    "print(\"%d ops in the final graph.\" % len(output_graph_def.node))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
